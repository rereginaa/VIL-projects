{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb80eb8-9f05-492d-973e-46bcb5bbf003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gym \n",
    "import random \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from matplotlib.pyplot import imshow, show\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd91aaf-180f-4400-99a1-1363c95b9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnNetwork(nn.Module):\n",
    "    def __init__(self, observation_dim, action_dim, hidden_dim):\n",
    "        super(RnnNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(observation_dim, hidden_dim)\n",
    "        self.gru = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, obs, hidden_state):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        hidden = hidden_state.reshape(-1, self.hidden_dim)\n",
    "        hidden = self.gru(x, hidden)\n",
    "        out = self.fc2(hidden)\n",
    "        return out, hidden\n",
    "\n",
    "class ReplayBufferRnn():\n",
    "    def __init__(self, buffer_size, episode_maxstep, observation_dim):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.episode_maxstep = episode_maxstep\n",
    "        self.observation_dim = observation_dim\n",
    "        self.current_idx = 0\n",
    "        self.current_size = 0\n",
    "  \n",
    "        self.memory = {\"o\": np.empty([self.buffer_size, self.episode_maxstep, self.observation_dim]),\n",
    "                       \"u\": np.empty([self.buffer_size, self.episode_maxstep, 1]),\n",
    "                       \"r\": np.empty([self.buffer_size, self.episode_maxstep, 1]),\n",
    "                       \"o_next\": np.empty([self.buffer_size, self.episode_maxstep, self.observation_dim]),\n",
    "                       \"padded\": np.empty([self.buffer_size, self.episode_maxstep, 1]),\n",
    "                       \"terminate\": np.empty([self.buffer_size, self.episode_maxstep, 1])}\n",
    "\n",
    "    def add(self, new_eps):\n",
    "        self.memory['o'][self.current_idx] = new_eps['o']\n",
    "        self.memory['u'][self.current_idx] = new_eps['u']\n",
    "        self.memory['r'][self.current_idx] = new_eps['r']\n",
    "        self.memory['o_next'][self.current_idx] = new_eps['o_next']\n",
    "        self.memory['padded'][self.current_idx] = new_eps['padded']\n",
    "        self.memory['terminate'][self.current_idx] = new_eps['terminate']\n",
    "\n",
    "        self.current_size = min(self.current_size + 1, self.buffer_size)\n",
    "        self.current_idx = 0 if self.current_idx == self.buffer_size - 1 else self.current_idx + 1\n",
    "\n",
    "    def sample(self, minibatch_size):\n",
    "        temp_buffer = {}\n",
    "        if self.current_size < minibatch_size:\n",
    "            return None\n",
    "        \n",
    "        idx = random.sample(range(self.current_size), minibatch_size)\n",
    "        for key in self.memory.keys():\n",
    "            temp_buffer[key] = self.memory[key][idx]\n",
    "\n",
    "        return temp_buffer\n",
    "\n",
    "class DRQNAgent(object):\n",
    "    def __init__(self, model, target_model, memory, gamma, batch_size, e, e_min, e_decay, update_freq, lr = 1e-3):\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "\n",
    "        self.hidden_state = None\n",
    "        self.target_hidden_state = None\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.memory = memory\n",
    "\n",
    "        self.e = e\n",
    "        self.e_min = e_min\n",
    "        self.e_decay = e_decay\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_step = 0\n",
    "        self.update_freq = update_freq\n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        self.hidden_state = torch.zeros((batch_size, 1, self.model.hidden_dim))\n",
    "        self.target_hidden_state = torch.zeros((batch_size, 1, self.target_model.hidden_dim))\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        q_value, self.hidden_state = self.model(obs, self.hidden_state)\n",
    "        if np.random.rand() <= self.e:\n",
    "            return random.randrange(len(q_value[0]))\n",
    "        else:\n",
    "            return torch.argmax(q_value[0]).item()\n",
    "\n",
    "    def get_inputs(self, batch, transition_idx):\n",
    "        obs, obs_next = batch['o'][:, transition_idx, :], batch['o_next'][:, transition_idx, :]\n",
    "        inputs = torch.tensor(obs, dtype=torch.float32)\n",
    "        inputs_next = torch.tensor(obs_next, dtype=torch.float32)\n",
    "        return inputs, inputs_next\n",
    "\n",
    "    def get_q_targetq_frombatch(self, batch, batch_max_step):\n",
    "        q_preds, q_targets = [], []\n",
    "        for timestep in range(batch_max_step):\n",
    "            inputs, inputs_next = self.get_inputs(batch, timestep)\n",
    "            q_pred, self.hidden_state = self.model(inputs, self.hidden_state)\n",
    "            q_target, self.target_hidden_state = self.target_model(inputs_next, self.target_hidden_state)\n",
    "            q_preds.append(q_pred)\n",
    "            q_targets.append(q_target)\n",
    "        q_preds = torch.stack(q_preds, dim=1)\n",
    "        q_targets = torch.stack(q_targets, dim=1)\n",
    "        return q_preds, q_targets\n",
    "\n",
    "    def _get_batch_max_step(self, batch):\n",
    "        terminated = batch['terminate']\n",
    "        episode_num = terminated.shape[0]\n",
    "        max_episode_len = 0\n",
    "        for episode_idx in range(episode_num):\n",
    "            for transition_idx in range(self.memory.episode_maxstep):\n",
    "                if terminated[episode_idx, transition_idx] == 1:\n",
    "                    if transition_idx + 1 >= max_episode_len:\n",
    "                        max_episode_len = transition_idx + 1\n",
    "                    break\n",
    "        if max_episode_len == 0:\n",
    "            max_episode_len = self.memory.episode_maxstep\n",
    "        return max_episode_len\n",
    "\n",
    "    def learn(self):\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        if batch is not None:\n",
    "            batch_max_step = self._get_batch_max_step(batch)\n",
    "\n",
    "            for key in batch.keys():\n",
    "                batch[key] = batch[key][:, :batch_max_step]\n",
    "            self.e = max(self.e_min, self.e * self.e_decay)\n",
    "            self.init_hidden_state(self.batch_size)\n",
    "\n",
    "            for key in batch.keys():\n",
    "                if key == 'u':\n",
    "                    batch[key] = torch.tensor(batch[key], dtype=torch.long)\n",
    "                else:\n",
    "                    batch[key] = torch.tensor(batch[key], dtype=torch.float32)\n",
    "\n",
    "            u, r, terminal = batch['u'], batch['r'], batch['terminate']\n",
    "           \n",
    "            mask = 1 - batch[\"padded\"].float()\n",
    "\n",
    "            q_preds, q_targets = self.get_q_targetq_frombatch(batch, batch_max_step)\n",
    "\n",
    "            q_preds = (torch.gather(q_preds, dim=2, index=u)).squeeze(1)\n",
    "            q_targets = (q_targets.max(dim=2)[0]).unsqueeze(2)\n",
    "\n",
    "            targets = r + self.gamma * q_targets * (1 - terminal)\n",
    "            td_error = (q_preds - targets.detach())\n",
    "\n",
    "            masked_td_error = mask * td_error\n",
    "            loss = (masked_td_error ** 2).sum() / mask.sum()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.train_step += 1\n",
    "            if self.train_step % self.update_freq:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38297ca-6564-4a00-8b63-cb16b33fa2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Rollout(object):\n",
    "    def __init__(self, env, agent, observation_shape):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.observation_shape = observation_shape\n",
    "\n",
    "    def generate_episode(self):\n",
    "        observation = self.env.reset()[::2]\n",
    "        o, o_next, u, r, terminate, padded = [], [], [], [], [], []\n",
    "        terminal = False\n",
    "        step = 0\n",
    "        self.agent.init_hidden_state(1)\n",
    "\n",
    "        while not terminal and step < self.agent.memory.episode_maxstep:\n",
    "            action = self.agent.act(observation)\n",
    "            next_observation, reward, terminal, _ = self.env.step(action)\n",
    "            next_observation = next_observation[::2]\n",
    "            o.append(observation)\n",
    "            o_next.append(next_observation)\n",
    "            observation = next_observation\n",
    "            u.append([action])\n",
    "            r.append([reward])\n",
    "            terminate.append([1.0 if terminal else 0.0])\n",
    "            padded.append([0.0])\n",
    "            step += 1\n",
    "\n",
    "        for i in range(step, self.agent.memory.episode_maxstep):\n",
    "            o.append(np.zeros((self.observation_shape)))\n",
    "            u.append(np.zeros((1)))\n",
    "            r.append([0.0])\n",
    "            o_next.append(np.zeros((self.observation_shape)))\n",
    "            padded.append([1.0])\n",
    "            terminate.append([1.0])\n",
    "\n",
    "        episode = dict(o=o.copy(),\n",
    "                       u=u.copy(),\n",
    "                       r=r.copy(),\n",
    "                       o_next=o_next.copy(),\n",
    "                       padded=padded.copy(),\n",
    "                       terminate=terminate.copy()\n",
    "                       )\n",
    "\n",
    "        #print(\"score :{}\".format(step))\n",
    "        return episode, step\n",
    "\n",
    "def main():\n",
    "    observation_dim = 2\n",
    "    hidden_dim = 32\n",
    "    action_dim = 2\n",
    "\n",
    "    drqn_net = RnnNetwork(observation_dim, action_dim, hidden_dim)\n",
    "    drqn_target_net = RnnNetwork(observation_dim, action_dim, hidden_dim)\n",
    "    drqn_target_net.load_state_dict(drqn_net.state_dict())\n",
    "\n",
    "    buffer_size = 10000\n",
    "    episode_maxstep = 500\n",
    "    replay_buffer = ReplayBufferRnn(buffer_size, episode_maxstep, observation_dim)\n",
    "\n",
    "    gamma = 0.9\n",
    "    minibatch_size = 2\n",
    "    e = 1.0\n",
    "    e_min = 0.01\n",
    "    e_decay = 0.99\n",
    "    update_freq = 10\n",
    "    drqn_agent = DRQNAgent(drqn_net, drqn_target_net, replay_buffer, gamma, minibatch_size, e, e_min, e_decay,\n",
    "                           update_freq)\n",
    "\n",
    "    env = gym.make('CartPole-v1')\n",
    "    rollout = Rollout(env, drqn_agent, observation_dim)\n",
    "\n",
    "    max_train_episodes = 10000\n",
    "    current_eps = 0\n",
    "    collect_n_episode = 1\n",
    "    scores = deque(maxlen=100)\n",
    "    moving_average = []\n",
    "    while current_eps <= max_train_episodes:\n",
    "        episode, step = rollout.generate_episode()\n",
    "        scores.append(step)\n",
    "        if current_eps % 100 == 0:\n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Epsilon {:.2}\".format(current_eps, np.mean(scores), np.max(scores), drqn_agent.e))\n",
    "        #print(\"episode {}/{}\".format(current_episode, max_train_episodes))\n",
    "        #print(\"epsilon {:.2}\".format(drqn_agent.e))\n",
    "        moving_average.append(np.mean(scores))\n",
    "        drqn_agent.memory.add(episode)\n",
    "        drqn_agent.learn()\n",
    "        current_eps += 1\n",
    "\n",
    "    plt.plot(moving_average)\n",
    "    plt.title('DRQN Solve Cartpole')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b026eca7-e334-44ec-af45-3508b14c80e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/regina/gym/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/regina/gym/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/var/folders/bk/km4jr97x4t3386lc8kvtzh300000gn/T/ipykernel_45188/834586320.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(obs, dtype=torch.float32)\n",
      "/var/folders/bk/km4jr97x4t3386lc8kvtzh300000gn/T/ipykernel_45188/834586320.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_next = torch.tensor(obs_next, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Average Reward 48.0 Best Reward 48 Epsilon 1.0\n",
      "Episode 100 Average Reward 16.84 Best Reward 69 Epsilon 0.37\n",
      "Episode 200 Average Reward 11.44 Best Reward 20 Epsilon 0.14\n",
      "Episode 300 Average Reward 12.51 Best Reward 24 Epsilon 0.05\n",
      "Episode 400 Average Reward 10.85 Best Reward 16 Epsilon 0.018\n",
      "Episode 500 Average Reward 10.93 Best Reward 15 Epsilon 0.01\n",
      "Episode 600 Average Reward 10.16 Best Reward 15 Epsilon 0.01\n",
      "Episode 700 Average Reward 11.14 Best Reward 17 Epsilon 0.01\n",
      "Episode 800 Average Reward 17.52 Best Reward 57 Epsilon 0.01\n",
      "Episode 900 Average Reward 24.68 Best Reward 51 Epsilon 0.01\n",
      "Episode 1000 Average Reward 22.59 Best Reward 50 Epsilon 0.01\n",
      "Episode 1100 Average Reward 24.31 Best Reward 50 Epsilon 0.01\n",
      "Episode 1200 Average Reward 22.19 Best Reward 49 Epsilon 0.01\n",
      "Episode 1300 Average Reward 18.86 Best Reward 45 Epsilon 0.01\n",
      "Episode 1400 Average Reward 15.65 Best Reward 49 Epsilon 0.01\n",
      "Episode 1500 Average Reward 26.37 Best Reward 58 Epsilon 0.01\n",
      "Episode 1600 Average Reward 27.21 Best Reward 65 Epsilon 0.01\n",
      "Episode 1700 Average Reward 45.44 Best Reward 88 Epsilon 0.01\n",
      "Episode 1800 Average Reward 39.74 Best Reward 84 Epsilon 0.01\n",
      "Episode 1900 Average Reward 36.67 Best Reward 67 Epsilon 0.01\n",
      "Episode 2000 Average Reward 32.86 Best Reward 65 Epsilon 0.01\n",
      "Episode 2100 Average Reward 39.71 Best Reward 79 Epsilon 0.01\n",
      "Episode 2200 Average Reward 32.5 Best Reward 68 Epsilon 0.01\n",
      "Episode 2300 Average Reward 32.07 Best Reward 60 Epsilon 0.01\n",
      "Episode 2400 Average Reward 23.72 Best Reward 62 Epsilon 0.01\n",
      "Episode 2500 Average Reward 22.67 Best Reward 62 Epsilon 0.01\n",
      "Episode 2600 Average Reward 21.76 Best Reward 50 Epsilon 0.01\n",
      "Episode 2700 Average Reward 24.6 Best Reward 49 Epsilon 0.01\n",
      "Episode 2800 Average Reward 27.47 Best Reward 54 Epsilon 0.01\n",
      "Episode 2900 Average Reward 24.87 Best Reward 53 Epsilon 0.01\n",
      "Episode 3000 Average Reward 27.22 Best Reward 51 Epsilon 0.01\n",
      "Episode 3100 Average Reward 27.04 Best Reward 48 Epsilon 0.01\n",
      "Episode 3200 Average Reward 29.74 Best Reward 53 Epsilon 0.01\n",
      "Episode 3300 Average Reward 35.93 Best Reward 65 Epsilon 0.01\n",
      "Episode 3400 Average Reward 35.75 Best Reward 63 Epsilon 0.01\n",
      "Episode 3500 Average Reward 36.3 Best Reward 63 Epsilon 0.01\n",
      "Episode 3600 Average Reward 40.96 Best Reward 76 Epsilon 0.01\n",
      "Episode 3700 Average Reward 37.9 Best Reward 81 Epsilon 0.01\n",
      "Episode 3800 Average Reward 43.67 Best Reward 79 Epsilon 0.01\n",
      "Episode 3900 Average Reward 49.4 Best Reward 106 Epsilon 0.01\n",
      "Episode 4000 Average Reward 43.9 Best Reward 79 Epsilon 0.01\n",
      "Episode 4100 Average Reward 57.84 Best Reward 159 Epsilon 0.01\n",
      "Episode 4200 Average Reward 70.09 Best Reward 179 Epsilon 0.01\n",
      "Episode 4300 Average Reward 68.98 Best Reward 179 Epsilon 0.01\n",
      "Episode 4400 Average Reward 88.75 Best Reward 332 Epsilon 0.01\n",
      "Episode 4500 Average Reward 87.62 Best Reward 257 Epsilon 0.01\n",
      "Episode 4600 Average Reward 116.4 Best Reward 409 Epsilon 0.01\n",
      "Episode 4700 Average Reward 107.38 Best Reward 347 Epsilon 0.01\n",
      "Episode 4800 Average Reward 132.63 Best Reward 394 Epsilon 0.01\n",
      "Episode 4900 Average Reward 131.65 Best Reward 376 Epsilon 0.01\n",
      "Episode 5000 Average Reward 131.56 Best Reward 391 Epsilon 0.01\n",
      "Episode 5100 Average Reward 155.73 Best Reward 500 Epsilon 0.01\n",
      "Episode 5200 Average Reward 151.91 Best Reward 365 Epsilon 0.01\n",
      "Episode 5300 Average Reward 151.98 Best Reward 328 Epsilon 0.01\n",
      "Episode 5400 Average Reward 154.24 Best Reward 340 Epsilon 0.01\n",
      "Episode 5500 Average Reward 167.02 Best Reward 380 Epsilon 0.01\n",
      "Episode 5600 Average Reward 132.3 Best Reward 459 Epsilon 0.01\n",
      "Episode 5700 Average Reward 192.73 Best Reward 460 Epsilon 0.01\n",
      "Episode 5800 Average Reward 163.61 Best Reward 396 Epsilon 0.01\n",
      "Episode 5900 Average Reward 162.66 Best Reward 420 Epsilon 0.01\n",
      "Episode 6000 Average Reward 142.76 Best Reward 411 Epsilon 0.01\n",
      "Episode 6100 Average Reward 134.97 Best Reward 349 Epsilon 0.01\n",
      "Episode 6200 Average Reward 122.27 Best Reward 314 Epsilon 0.01\n",
      "Episode 6300 Average Reward 135.45 Best Reward 286 Epsilon 0.01\n",
      "Episode 6400 Average Reward 122.7 Best Reward 246 Epsilon 0.01\n",
      "Episode 6500 Average Reward 106.9 Best Reward 260 Epsilon 0.01\n",
      "Episode 6600 Average Reward 140.5 Best Reward 411 Epsilon 0.01\n",
      "Episode 6700 Average Reward 114.59 Best Reward 251 Epsilon 0.01\n",
      "Episode 6800 Average Reward 144.19 Best Reward 418 Epsilon 0.01\n",
      "Episode 6900 Average Reward 130.89 Best Reward 378 Epsilon 0.01\n",
      "Episode 7000 Average Reward 151.62 Best Reward 463 Epsilon 0.01\n",
      "Episode 7100 Average Reward 155.91 Best Reward 485 Epsilon 0.01\n",
      "Episode 7200 Average Reward 163.05 Best Reward 408 Epsilon 0.01\n",
      "Episode 7300 Average Reward 120.85 Best Reward 275 Epsilon 0.01\n",
      "Episode 7400 Average Reward 162.12 Best Reward 460 Epsilon 0.01\n",
      "Episode 7500 Average Reward 159.22 Best Reward 425 Epsilon 0.01\n",
      "Episode 7600 Average Reward 152.97 Best Reward 427 Epsilon 0.01\n",
      "Episode 7700 Average Reward 183.65 Best Reward 466 Epsilon 0.01\n",
      "Episode 7800 Average Reward 137.12 Best Reward 434 Epsilon 0.01\n",
      "Episode 7900 Average Reward 132.52 Best Reward 346 Epsilon 0.01\n",
      "Episode 8000 Average Reward 200.93 Best Reward 500 Epsilon 0.01\n",
      "Episode 8100 Average Reward 243.74 Best Reward 470 Epsilon 0.01\n",
      "Episode 8200 Average Reward 187.89 Best Reward 453 Epsilon 0.01\n",
      "Episode 8300 Average Reward 169.63 Best Reward 449 Epsilon 0.01\n",
      "Episode 8400 Average Reward 264.34 Best Reward 500 Epsilon 0.01\n",
      "Episode 8500 Average Reward 170.32 Best Reward 432 Epsilon 0.01\n",
      "Episode 8600 Average Reward 199.62 Best Reward 500 Epsilon 0.01\n",
      "Episode 8700 Average Reward 191.99 Best Reward 500 Epsilon 0.01\n",
      "Episode 8800 Average Reward 226.92 Best Reward 500 Epsilon 0.01\n",
      "Episode 8900 Average Reward 231.46 Best Reward 500 Epsilon 0.01\n",
      "Episode 9000 Average Reward 169.9 Best Reward 409 Epsilon 0.01\n",
      "Episode 9100 Average Reward 179.6 Best Reward 420 Epsilon 0.01\n",
      "Episode 9200 Average Reward 165.77 Best Reward 459 Epsilon 0.01\n",
      "Episode 9300 Average Reward 227.49 Best Reward 500 Epsilon 0.01\n",
      "Episode 9400 Average Reward 248.39 Best Reward 500 Epsilon 0.01\n",
      "Episode 9500 Average Reward 191.18 Best Reward 500 Epsilon 0.01\n",
      "Episode 9600 Average Reward 339.92 Best Reward 500 Epsilon 0.01\n",
      "Episode 9700 Average Reward 294.85 Best Reward 500 Epsilon 0.01\n",
      "Episode 9800 Average Reward 252.17 Best Reward 500 Epsilon 0.01\n",
      "Episode 9900 Average Reward 282.2 Best Reward 500 Epsilon 0.01\n",
      "Episode 10000 Average Reward 226.66 Best Reward 500 Epsilon 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1999aeef-3d9d-486b-b82b-2aa7c8b00ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bk/km4jr97x4t3386lc8kvtzh300000gn/T/ipykernel_45188/3385844940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Log the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards per episodes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gym\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Q_network\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self, state_space=None,\n",
    "                 action_space=None):\n",
    "        super(Q_net, self).__init__()\n",
    "\n",
    "        # space size check\n",
    "        assert state_space is not None, \"None state_space input: state_space should be selected.\"\n",
    "        assert action_space is not None, \"None action_space input: action_space should be selected.\"\n",
    "\n",
    "        self.hidden_space = 64\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.Linear1 = nn.Linear(self.state_space, self.hidden_space)\n",
    "        self.lstm    = nn.LSTM(self.hidden_space,self.hidden_space, batch_first=True)\n",
    "        self.Linear2 = nn.Linear(self.hidden_space, self.action_space)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x = F.relu(self.Linear1(x))\n",
    "        x, (new_h, new_c) = self.lstm(x,(h,c))\n",
    "        x = self.Linear2(x)\n",
    "        return x, new_h, new_c\n",
    "\n",
    "    def sample_action(self, obs, h,c, epsilon):\n",
    "        output = self.forward(obs, h,c)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0,1), output[1], output[2]\n",
    "        else:\n",
    "            return output[0].argmax().item(), output[1] , output[2]\n",
    "    \n",
    "    def init_hidden_state(self, batch_size, training=None):\n",
    "\n",
    "        assert training is not None, \"training step parameter should be dtermined\"\n",
    "\n",
    "        if training is True:\n",
    "            return torch.zeros([1, batch_size, self.hidden_space]), torch.zeros([1, batch_size, self.hidden_space])\n",
    "        else:\n",
    "            return torch.zeros([1, 1, self.hidden_space]), torch.zeros([1, 1, self.hidden_space])\n",
    "\n",
    "class EpisodeMemory():\n",
    "    \"\"\"Episode memory for recurrent agent\"\"\"\n",
    "\n",
    "    def __init__(self, random_update=False, \n",
    "                       max_epi_num=100, max_epi_len=500,\n",
    "                       batch_size=1,\n",
    "                       lookup_step=None):\n",
    "        self.random_update = random_update # if False, sequential update\n",
    "        self.max_epi_num = max_epi_num\n",
    "        self.max_epi_len = max_epi_len\n",
    "        self.batch_size = batch_size\n",
    "        self.lookup_step = lookup_step\n",
    "\n",
    "        if (random_update is False) and (self.batch_size > 1):\n",
    "            sys.exit('It is recommend to use 1 batch for sequential update, if you want, erase this code block and modify code')\n",
    "\n",
    "        self.memory = collections.deque(maxlen=self.max_epi_num)\n",
    "\n",
    "    def put(self, episode):\n",
    "        self.memory.append(episode)\n",
    "\n",
    "    def sample(self):\n",
    "        sampled_buffer = []\n",
    "\n",
    "        ##################### RANDOM UPDATE ############################\n",
    "        if self.random_update: # Random upodate\n",
    "            sampled_episodes = random.sample(self.memory, self.batch_size)\n",
    "            \n",
    "            check_flag = True # check if every sample data to train is larger than batch size\n",
    "            min_step = self.max_epi_len\n",
    "\n",
    "            for episode in sampled_episodes:\n",
    "                min_step = min(min_step, len(episode)) # get minimum step from sampled episodes\n",
    "\n",
    "            for episode in sampled_episodes:\n",
    "                if min_step > self.lookup_step: # sample buffer with lookup_step size\n",
    "                    idx = np.random.randint(0, len(episode)-self.lookup_step+1)\n",
    "                    sample = episode.sample(random_update=self.random_update, lookup_step=self.lookup_step, idx=idx)\n",
    "                    sampled_buffer.append(sample)\n",
    "                else:\n",
    "                    idx = np.random.randint(0, len(episode)-min_step+1) # sample buffer with minstep size\n",
    "                    sample = episode.sample(random_update=self.random_update, lookup_step=min_step, idx=idx)\n",
    "                    sampled_buffer.append(sample)\n",
    "\n",
    "        ##################### SEQUENTIAL UPDATE ############################           \n",
    "        else: # Sequential update\n",
    "            idx = np.random.randint(0, len(self.memory))\n",
    "            sampled_buffer.append(self.memory[idx].sample(random_update=self.random_update))\n",
    "\n",
    "        return sampled_buffer, len(sampled_buffer[0]['obs']) # buffers, sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class EpisodeBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.action = []\n",
    "        self.reward = []\n",
    "        self.next_obs = []\n",
    "        self.done = []\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.obs.append(transition[0])\n",
    "        self.action.append(transition[1])\n",
    "        self.reward.append(transition[2])\n",
    "        self.next_obs.append(transition[3])\n",
    "        self.done.append(transition[4])\n",
    "\n",
    "    def sample(self, random_update=False, lookup_step=None, idx=None) -> Dict[str, np.ndarray]:\n",
    "        obs = np.array(self.obs)\n",
    "        action = np.array(self.action)\n",
    "        reward = np.array(self.reward)\n",
    "        next_obs = np.array(self.next_obs)\n",
    "        done = np.array(self.done)\n",
    "\n",
    "        if random_update is True:\n",
    "            obs = obs[idx:idx+lookup_step]\n",
    "            action = action[idx:idx+lookup_step]\n",
    "            reward = reward[idx:idx+lookup_step]\n",
    "            next_obs = next_obs[idx:idx+lookup_step]\n",
    "            done = done[idx:idx+lookup_step]\n",
    "\n",
    "        return dict(obs=obs,\n",
    "                    acts=action,\n",
    "                    rews=reward,\n",
    "                    next_obs=next_obs,\n",
    "                    done=done)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.obs)\n",
    "\n",
    "\n",
    "def train(q_net=None, target_q_net=None, episode_memory=None,\n",
    "          device=None, \n",
    "          optimizer = None,\n",
    "          batch_size=1,\n",
    "          learning_rate=1e-3,\n",
    "          gamma=0.99):\n",
    "\n",
    "    assert device is not None, \"None Device input: device should be selected.\"\n",
    "\n",
    "    # Get batch from replay buffer\n",
    "    samples, seq_len = episode_memory.sample()\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_observations = []\n",
    "    dones = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        observations.append(samples[i][\"obs\"])\n",
    "        actions.append(samples[i][\"acts\"])\n",
    "        rewards.append(samples[i][\"rews\"])\n",
    "        next_observations.append(samples[i][\"next_obs\"])\n",
    "        dones.append(samples[i][\"done\"])\n",
    "\n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    next_observations = np.array(next_observations)\n",
    "    dones = np.array(dones)\n",
    "\n",
    "    observations = torch.FloatTensor(observations.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    actions = torch.LongTensor(actions.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    rewards = torch.FloatTensor(rewards.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    next_observations = torch.FloatTensor(next_observations.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    dones = torch.FloatTensor(dones.reshape(batch_size,seq_len,-1)).to(device)\n",
    "\n",
    "    h_target, c_target = target_q_net.init_hidden_state(batch_size=batch_size, training=True)\n",
    "\n",
    "    q_target, _, _ = target_q_net(next_observations, h_target.to(device), c_target.to(device))\n",
    "\n",
    "    q_target_max = q_target.max(2)[0].view(batch_size,seq_len,-1).detach()\n",
    "    targets = rewards + gamma*q_target_max*dones\n",
    "\n",
    "\n",
    "    h, c = q_net.init_hidden_state(batch_size=batch_size, training=True)\n",
    "    q_out, _, _ = q_net(observations, h.to(device), c.to(device))\n",
    "    q_a = q_out.gather(2, actions)\n",
    "\n",
    "    # Multiply Importance Sampling weights to loss        \n",
    "    loss = F.smooth_l1_loss(q_a, targets)\n",
    "    \n",
    "    # Update Network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Env parameters\n",
    "    model_name = \"DRQN_POMDP_Random\"\n",
    "    env_name = \"CartPole-v1\"\n",
    "    seed = 1\n",
    "    exp_num = 'SEED'+'_'+str(seed)\n",
    "\n",
    "    # Set gym environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "\n",
    "    # Set parameters\n",
    "    batch_size = 8\n",
    "    learning_rate = 1e-3\n",
    "    buffer_len = int(100000)\n",
    "    min_epi_num = 20 # Start moment to train the Q network\n",
    "    episodes = 1000\n",
    "    print_per_iter = 100\n",
    "    target_update_period = 4\n",
    "    eps_start = 0.1\n",
    "    eps_end = 0.001\n",
    "    eps_decay = 0.995\n",
    "    tau = 1e-2\n",
    "    max_step = 2000\n",
    "\n",
    "    # DRQN param\n",
    "    random_update = True# If you want to do random update instead of sequential update\n",
    "    lookup_step = 20 # If you want to do random update instead of sequential update\n",
    "    max_epi_len = 100 \n",
    "    max_epi_step = max_step\n",
    "\n",
    "    \n",
    "\n",
    "    # Create Q functions\n",
    "    Q = Q_net(state_space=env.observation_space.shape[0]-2, \n",
    "              action_space=env.action_space.n).to(device)\n",
    "    Q_target = Q_net(state_space=env.observation_space.shape[0]-2, \n",
    "                     action_space=env.action_space.n).to(device)\n",
    "\n",
    "    Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "    # Set optimizer\n",
    "    score = 0\n",
    "    score_sum = 0\n",
    "    optimizer = optim.Adam(Q.parameters(), lr=learning_rate)\n",
    "\n",
    "    epsilon = eps_start\n",
    "    \n",
    "    episode_memory = EpisodeMemory(random_update=random_update, \n",
    "                                   max_epi_num=100, max_epi_len=600, \n",
    "                                   batch_size=batch_size, \n",
    "                                   lookup_step=lookup_step)\n",
    "\n",
    "    # Train\n",
    "    for i in range(episodes):\n",
    "        s = env.reset()\n",
    "        obs = s[::2] # Use only Position of Cart and Pole\n",
    "        done = False\n",
    "        \n",
    "        episode_record = EpisodeBuffer()\n",
    "        h, c = Q.init_hidden_state(batch_size=batch_size, training=False)\n",
    "\n",
    "        for t in range(max_step):\n",
    "\n",
    "            # Get action\n",
    "            a, h, c = Q.sample_action(torch.from_numpy(obs).float().to(device).unsqueeze(0).unsqueeze(0), \n",
    "                                              h.to(device), c.to(device),\n",
    "                                              epsilon)\n",
    "\n",
    "            # Do action\n",
    "            s_prime, r, done, _ = env.step(a)\n",
    "            obs_prime = s_prime[::2]\n",
    "\n",
    "            # make data\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "\n",
    "            episode_record.put([obs, a, r/100.0, obs_prime, done_mask])\n",
    "\n",
    "            obs = obs_prime\n",
    "            \n",
    "            score += r\n",
    "            score_sum += r\n",
    "\n",
    "            if len(episode_memory) >= min_epi_num:\n",
    "                train(Q, Q_target, episode_memory, device, \n",
    "                        optimizer=optimizer,\n",
    "                        batch_size=batch_size,\n",
    "                        learning_rate=learning_rate)\n",
    "\n",
    "                if (t+1) % target_update_period == 0:\n",
    "                    # Q_target.load_state_dict(Q.state_dict()) <- navie update\n",
    "                    for target_param, local_param in zip(Q_target.parameters(), Q.parameters()): # <- soft update\n",
    "                            target_param.data.copy_(tau*local_param.data + (1.0 - tau)*target_param.data)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_memory.put(episode_record)\n",
    "        \n",
    "        epsilon = max(eps_end, epsilon * eps_decay) #Linear annealing\n",
    "\n",
    "        if i % print_per_iter == 0 and i!=0:\n",
    "            print(\"Episode: {}, Average Reward: {:.1f} Epsilon: {:.2}\".format(i, score_sum/print_per_iter, epsilon))\n",
    "            score_sum=0.0\n",
    "            save_model(Q, model_name+\"_\"+exp_num+'.pth')\n",
    "\n",
    "        # Log the reward\n",
    "        writer.add_scalar('Rewards per episodes', score, i)\n",
    "        score = 0\n",
    "        \n",
    "    writer.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22b709-8151-4cbf-9481-e05f72999010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
